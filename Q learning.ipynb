{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97dce307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf8a65",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1335924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "max_steps_per_episode = 100\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "exploration_rate = 1.0\n",
    "max_exploration_rate = 1.0\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf162f8",
   "metadata": {},
   "source": [
    "## environmental set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973fd0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = 4   # Actions: 0=up, 1=right, 2=down, 3=left\n",
    "state_space_size = 16   # Assume a 4x4 grid environment\n",
    "q_table = np.zeros((state_space_size, action_space_size))  # Initialize Q-table with zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1429d6a5",
   "metadata": {},
   "source": [
    "## rewards set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de2dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.zeros(state_space_size)\n",
    "rewards[15] = 100  # Goal state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ae24b5",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6dda8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    \"\"\"Get the next state given the current state and action.\"\"\"\n",
    "    row = state // 4\n",
    "    col = state % 4\n",
    "    if action == 0 and row > 0:        # Up\n",
    "        row -= 1\n",
    "    elif action == 1 and col < 3:      # Right\n",
    "        col += 1\n",
    "    elif action == 2 and row < 3:      # Down\n",
    "        row += 1\n",
    "    elif action == 3 and col > 0:      # Left\n",
    "        col -= 1\n",
    "    return row * 4 + col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60f06a3",
   "metadata": {},
   "source": [
    "## Q learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76aca230",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = np.random.randint(0, state_space_size - 1)  # Start in random state\n",
    "    done = False\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Exploration-exploitation trade-off\n",
    "        if random.uniform(0, 1) < exploration_rate:\n",
    "            action = np.random.randint(action_space_size)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])          # Exploit\n",
    "\n",
    "        # Take action and observe reward and next state\n",
    "        new_state = get_next_state(state, action)\n",
    "        reward = rewards[new_state]\n",
    "\n",
    "        # Update Q-table\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "            reward + discount_rate * np.max(q_table[new_state, :]) - q_table[state, action]\n",
    "        )\n",
    "\n",
    "        state = new_state  # Transition to next state\n",
    "\n",
    "        if state == 15:  # Check if the goal is reached\n",
    "            break\n",
    "\n",
    "    # Decay exploration rate\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d9cd15",
   "metadata": {},
   "source": [
    "## Output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a3cc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-table:\n",
      "[[ 93.43416364  95.09874983  95.0561567   94.02932763]\n",
      " [ 95.06553942  96.05960098  96.03791914  93.94434586]\n",
      " [ 96.05489684  97.02784086  97.0299      95.09195475]\n",
      " [ 96.98972644  97.01823599  98.00999999  96.00863178]\n",
      " [ 94.03185513  95.97330536  96.05960042  95.01799281]\n",
      " [ 95.06246094  97.02989997  97.02186634  95.08459314]\n",
      " [ 96.05958389  98.00999878  98.01        96.05955016]\n",
      " [ 97.02954427  98.00996767  99.          97.02981783]\n",
      " [ 95.0795032   97.0299      97.02716665  96.05565288]\n",
      " [ 96.05781661  98.01        98.00973942  96.05942383]\n",
      " [ 97.02989998  99.          98.99999997  97.02989975]\n",
      " [ 98.00999999  98.99999995 100.          98.00999995]\n",
      " [ 96.04336613  98.00999977  97.0241132   96.99592822]\n",
      " [ 97.02816579  99.          98.00908358  97.02807474]\n",
      " [ 98.00985287 100.          98.99993894  98.00883666]\n",
      " [  0.           0.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained Q-table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2191ae",
   "metadata": {},
   "source": [
    "Environment and Q-Table Initialization: Assuming a simple 4x4 grid (16 states), with the goal state in the bottom-right corner (state 15) that gives a reward of 100.\n",
    "\n",
    "Q-Learning Algorithm:\n",
    "For each episode, the agent starts in a random state.\n",
    "It selects actions based on an epsilon-greedy policy(explores with prob epsilon and exploits with prob 1-epsilon)\n",
    "The Q-value update rule is applied after each action.\n",
    "\n",
    "Exploration Decay: The exploration rate gradually decreases with each episode to encourage the agent to exploit learned knowledge over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279071c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
